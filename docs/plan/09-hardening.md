# 09 - Hardening & Performance

**Goal:** Close down abuse layers, finalize rate limit restrictions, and confirm application stability under simulated loads.

## Definition of Done

- Endpoint burst limit configurations.
- Idempotency confirmed.
- Tested Load limits via `k6` or `Locust`.

## Implementation Steps

1. **Rate Limiting Setup:**
   - Integrate the `slowapi` library or wrap standard Redis-based bucketing (`RATE_LIMIT_PER_MINUTE=60` env variable).
   - Rate limit by IP for anonymous endpoint (`GET /oauth/authorize`).
   - Rate limit by `user_id` inside the JWT for all downstream action calls (`/pets/*`).
   - Ensure an HTTP `429 Too Many Requests` response is raised for the GPT to consume directly, bypassing the `502 Upstream` normalizer if it is generated by our connector.

2. **Idempotency Final Checks:**
   - Test double-clicking "Create" on `POST /pets`. Our `find` check stops semantic identicals. Ensure standard POST endpoints act similarly if needed or if HTTP mappings strictly use `PATCH` / `PUT`.

3. **Token Revocation Blacklists:**
   - If the upstream main app hasn't finalized `POST /api/gpt-auth/revoke`, or as an additional line of defense, blacklist revoked/used one-time JWT tokens in the connector tier using Redis sets.

4. **Load & Alert Baseline:**
   - Use `locust` (or a short automated script simulating multiple tokens) hitting `/pets` constantly.
   - Confirm limits block the surge without crashing `uvicorn`.
   - Guarantee `GET /health` remains fast and returns `200` regardless of heavy `/pets` traffic loops.
